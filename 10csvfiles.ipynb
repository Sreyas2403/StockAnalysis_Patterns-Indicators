{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2+TZKsbszwFZywM/T0NeC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sreyas2403/StockAnalysis_Patterns-Indicators/blob/main/10csvfiles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dL-T2oBF7gn3",
        "outputId": "ffa06766-938c-4f67-bc03-b247e2e56872"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nsepython\n",
            "  Downloading nsepython-2.8-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from nsepython) (2.31.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from nsepython) (2.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from nsepython) (1.11.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->nsepython) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->nsepython) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->nsepython) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->nsepython) (1.25.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->nsepython) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->nsepython) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->nsepython) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->nsepython) (2024.6.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->nsepython) (1.16.0)\n",
            "Installing collected packages: nsepython\n",
            "Successfully installed nsepython-2.8\n"
          ]
        }
      ],
      "source": [
        "!pip install nsepython"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nsepython as nse\n",
        "import pandas as pd\n",
        "\n",
        "def equity_history(symbol, series, start_date, end_date):\n",
        "    # Fetch equity history for the selected symbol\n",
        "    return nse.equity_history(symbol, series, start_date, end_date)\n",
        "\n",
        "def calculate_rsi(data, window):\n",
        "    delta = data['CH_CLOSING_PRICE'].diff()\n",
        "    gain = (delta.where(delta > 0, 0)).fillna(0)\n",
        "    loss = (-delta.where(delta < 0, 0)).fillna(0)\n",
        "    avg_gain = gain.rolling(window=window, min_periods=1).mean()\n",
        "    avg_loss = loss.rolling(window=window, min_periods=1).mean()\n",
        "    rs = avg_gain / avg_loss\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    return rsi\n",
        "\n",
        "# Define the stock symbol and date range\n",
        "symbol = input(\"Enter the symbol: \")\n",
        "series = \"EQ\"\n",
        "start_date = input(\"Enter the start date (yyyy-mm-dd): \")\n",
        "end_date = input(\"Enter the end date (yyyy-mm-dd): \")\n",
        "\n",
        "# Fetch equity data\n",
        "equity_data = equity_history(symbol, series, start_date, end_date)\n",
        "stock_df = pd.DataFrame(equity_data)\n",
        "stock_df['mTIMESTAMP'] = pd.to_datetime(stock_df['mTIMESTAMP'])\n",
        "stock_df = stock_df.sort_values(by='mTIMESTAMP')\n",
        "\n",
        "# Convert necessary columns to numeric\n",
        "stock_df['CH_CLOSING_PRICE'] = pd.to_numeric(stock_df['CH_CLOSING_PRICE'], errors='coerce')\n",
        "\n",
        "# Drop rows with NaN values in key columns after conversion\n",
        "stock_df = stock_df.dropna(subset=['CH_CLOSING_PRICE'])\n",
        "\n",
        "# Calculate RSI\n",
        "window_length = 14\n",
        "stock_df['RSI'] = calculate_rsi(stock_df, window_length)\n",
        "\n",
        "# Identify RSI values in valleys and peaks\n",
        "prospect_list_valleys = []\n",
        "prospect_list_peaks = []\n",
        "\n",
        "for i in range(window_length, len(stock_df) - window_length):\n",
        "    rsi_value = stock_df['RSI'].iloc[i]\n",
        "    left_values = stock_df['RSI'].iloc[i-window_length:i]\n",
        "    right_values = stock_df['RSI'].iloc[i+1:i+1+window_length]\n",
        "\n",
        "    if all(abs(rsi_value - lv) > 1.5 for lv in left_values) and all(abs(rsi_value - rv) > 1.5 for rv in right_values):\n",
        "        prospect_list_valleys.append(rsi_value)\n",
        "\n",
        "    if all(abs(rsi_value - lv) > 1.5 for lv in left_values) and all(abs(rsi_value - rv) > 1.5 for rv in right_values):\n",
        "        prospect_list_peaks.append(rsi_value)\n",
        "\n",
        "# Print the prospect lists\n",
        "print(\"Prospect List in Valleys (RSI values):\")\n",
        "print(prospect_list_valleys)\n",
        "\n",
        "print(\"\\nProspect List in Peaks (RSI values):\")\n",
        "print(prospect_list_peaks)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mPgw8Uk7jU8",
        "outputId": "b1cdc73c-cca8-48b4-dcb3-7fea4854c037"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the symbol: SBIN\n",
            "Enter the start date (yyyy-mm-dd): 01-01-2021\n",
            "Enter the end date (yyyy-mm-dd): 31-12-2023\n",
            "Prospect List in Valleys (RSI values):\n",
            "[51.73542894564504, 49.48979591836734, 44.15422885572137, 81.03225806451611, 79.19921874999997, 88.89294848374129, 95.0892857142857, 70.926143024619, 44.55825864276569, 23.743922204213817, 56.486622917718314, 78.74080130825843, 77.21297107800181, 68.72074882995322, 66.71974522292994, 95.41284403669725, 78.91891891891895, 73.178391959799, 71.05084745762714, 66.05723370429254, 46.45328719723181, 39.152371342078744, 37.415621986499545, 40.873015873015866, 44.454799627213404, 42.760942760942775, 60.07827788649706, 32.5693606755127, 22.42547425474261, 25.778816199376976, 30.569514237855984, 42.09748892171346, 72.64705882352945, 95.76490924805542, 46.72379032258065, 52.10409745293464, 41.62265962554011, 30.011587485515662, 26.829268292682897, 23.345123799898914, 54.04089581304772, 56.0271646859083, 29.29542645241041, 42.4160346695558, 76.89873417721519, 11.902614968439991, 21.541155866900112, 44.994016753091366, 47.745358090185675, 53.195164075993105, 75.74316290130803, 74.23954372623578, 54.87214927436076, 47.09388971684055, 41.72154280338666, 43.967935871743485, 63.02998965873839, 67.34693877551018, 30.166787527193677, 41.76100628930817, 48.752834467120195, 52.35876528829351, 86.90884167237833, 71.09243697478993, 74.22771403353937, 76.03833865814696, 63.81538461538462, 40.458869613877994, 50.321336760925455, 65.65721649484539, 71.42051860202935, 69.89596879063717, 22.631578947368425, 34.79394887845589, 53.439153439153436, 66.92169216921695, 63.51986330627938, 61.25226860254083, 32.90414878397718, 28.41260037059918, 58.79458794587949, 63.43187660668374, 18.409090909090978, 46.990740740740755, 60.02132196162042, 58.493506493506494, 42.923867351704764, 40.69973427812222, 23.468862583633452, 28.498985801217003, 37.48674443266168, 64.58739441195587, 81.75824175824172, 83.59173126614984, 64.55142231947482, 47.629796839729124, 58.52534562211986, 63.788027477919556, 40.57826520438681, 17.465069860279442, 34.125636672325896, 55.25502318392587, 79.00318133616109, 56.85224839400432, 53.04695304695306, 28.747072599531663, 51.69738118331717, 44.55869751499572, 48.58611825192804, 74.67057101024895, 42.9460580912863, 44.483362521891394, 50.13157894736845, 29.963680387409198, 31.697054698457222, 34.059549745824256, 61.65011459129109, 87.45910577971665, 83.5948644793155, 81.32045088566832, 28.7610619469027, 24.064171122994665, 42.66792809839169, 59.0540540540541]\n",
            "\n",
            "Prospect List in Peaks (RSI values):\n",
            "[51.73542894564504, 49.48979591836734, 44.15422885572137, 81.03225806451611, 79.19921874999997, 88.89294848374129, 95.0892857142857, 70.926143024619, 44.55825864276569, 23.743922204213817, 56.486622917718314, 78.74080130825843, 77.21297107800181, 68.72074882995322, 66.71974522292994, 95.41284403669725, 78.91891891891895, 73.178391959799, 71.05084745762714, 66.05723370429254, 46.45328719723181, 39.152371342078744, 37.415621986499545, 40.873015873015866, 44.454799627213404, 42.760942760942775, 60.07827788649706, 32.5693606755127, 22.42547425474261, 25.778816199376976, 30.569514237855984, 42.09748892171346, 72.64705882352945, 95.76490924805542, 46.72379032258065, 52.10409745293464, 41.62265962554011, 30.011587485515662, 26.829268292682897, 23.345123799898914, 54.04089581304772, 56.0271646859083, 29.29542645241041, 42.4160346695558, 76.89873417721519, 11.902614968439991, 21.541155866900112, 44.994016753091366, 47.745358090185675, 53.195164075993105, 75.74316290130803, 74.23954372623578, 54.87214927436076, 47.09388971684055, 41.72154280338666, 43.967935871743485, 63.02998965873839, 67.34693877551018, 30.166787527193677, 41.76100628930817, 48.752834467120195, 52.35876528829351, 86.90884167237833, 71.09243697478993, 74.22771403353937, 76.03833865814696, 63.81538461538462, 40.458869613877994, 50.321336760925455, 65.65721649484539, 71.42051860202935, 69.89596879063717, 22.631578947368425, 34.79394887845589, 53.439153439153436, 66.92169216921695, 63.51986330627938, 61.25226860254083, 32.90414878397718, 28.41260037059918, 58.79458794587949, 63.43187660668374, 18.409090909090978, 46.990740740740755, 60.02132196162042, 58.493506493506494, 42.923867351704764, 40.69973427812222, 23.468862583633452, 28.498985801217003, 37.48674443266168, 64.58739441195587, 81.75824175824172, 83.59173126614984, 64.55142231947482, 47.629796839729124, 58.52534562211986, 63.788027477919556, 40.57826520438681, 17.465069860279442, 34.125636672325896, 55.25502318392587, 79.00318133616109, 56.85224839400432, 53.04695304695306, 28.747072599531663, 51.69738118331717, 44.55869751499572, 48.58611825192804, 74.67057101024895, 42.9460580912863, 44.483362521891394, 50.13157894736845, 29.963680387409198, 31.697054698457222, 34.059549745824256, 61.65011459129109, 87.45910577971665, 83.5948644793155, 81.32045088566832, 28.7610619469027, 24.064171122994665, 42.66792809839169, 59.0540540540541]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nsepython as nse\n",
        "import numpy as np\n",
        "\n",
        "# Define the stock symbol and series\n",
        "symbol = input(\"Enter the symbol: \")\n",
        "series = \"EQ\"\n",
        "start_date = input(\"Enter the start date (dd-mm-yyyy): \")\n",
        "end_date = input(\"Enter the end date (dd-mm-yyyy): \")\n",
        "\n",
        "# Fetch equity history for the selected symbol\n",
        "equity_data = nse.equity_history(symbol, series, start_date, end_date)\n",
        "stock_df = pd.DataFrame(equity_data)\n",
        "stock_df['mTIMESTAMP'] = pd.to_datetime(stock_df['mTIMESTAMP'])\n",
        "stock_df = stock_df.sort_values(by='mTIMESTAMP')\n",
        "\n",
        "# Convert necessary columns to numeric\n",
        "stock_df['CH_CLOSING_PRICE'] = pd.to_numeric(stock_df['CH_CLOSING_PRICE'], errors='coerce')\n",
        "\n",
        "# Drop rows with NaN values in key columns after conversion\n",
        "stock_df = stock_df.dropna(subset=['CH_CLOSING_PRICE'])\n",
        "\n",
        "# Calculate the RSI (Relative Strength Index)\n",
        "def calculate_rsi(data, window=14):\n",
        "    delta = data['CH_CLOSING_PRICE'].diff(1)\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
        "    rs = gain / loss\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    return rsi\n",
        "\n",
        "stock_df['RSI'] = calculate_rsi(stock_df)\n",
        "\n",
        "# Identify RSI prospect lists for valleys and peaks\n",
        "prospect_list_valleys = []\n",
        "prospect_list_peaks = []\n",
        "\n",
        "# Iterate over the RSI values\n",
        "for i in range(10, len(stock_df) - 10):\n",
        "    rsi_value = stock_df['RSI'].iloc[i]\n",
        "    left_rsi_values = stock_df['RSI'].iloc[i-10:i]\n",
        "    right_rsi_values = stock_df['RSI'].iloc[i+1:i+11]\n",
        "\n",
        "    # Check for valleys\n",
        "    if all(left_rsi_values > rsi_value + 1.5) and all(right_rsi_values > rsi_value + 1.5):\n",
        "        prospect_list_valleys.append(rsi_value)\n",
        "\n",
        "    # Check for peaks\n",
        "    if all(left_rsi_values < rsi_value - 1.5) and all(right_rsi_values < rsi_value - 1.5):\n",
        "        prospect_list_peaks.append(rsi_value)\n",
        "\n",
        "# Print the prospect lists\n",
        "print(\"RSI Prospect List in Valleys:\")\n",
        "print(prospect_list_valleys)\n",
        "\n",
        "print(\"RSI Prospect List in Peaks:\")\n",
        "print(prospect_list_peaks)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKXnuahCAy19",
        "outputId": "c09b8402-4bf3-4857-c2b5-4108c5231ecf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the symbol: SBIN\n",
            "Enter the start date (dd-mm-yyyy): 01-01-2021\n",
            "Enter the end date (dd-mm-yyyy): 31-12-2023\n",
            "RSI Prospect List in Valleys:\n",
            "[23.743922204213817, 42.760942760942775, 22.42547425474261, 46.72379032258065, 11.902614968439991, 22.631578947368425, 28.41260037059918, 18.409090909090978, 47.629796839729124, 17.465069860279442, 24.064171122994665]\n",
            "RSI Prospect List in Peaks:\n",
            "[95.0892857142857, 95.41284403669725, 95.76490924805542, 56.0271646859083, 80.75268817204307, 80.00000000000003, 67.34693877551018, 86.90884167237833, 63.43187660668374, 83.59173126614984, 74.67057101024895, 87.45910577971665]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prospect_list_valleys_df = pd.DataFrame(prospect_list_valleys)\n",
        "prospect_list_peaks_df = pd.DataFrame(prospect_list_peaks)\n",
        "\n",
        "#Saving to excel files\n",
        "prospect_list_valleys_df.to_excel('prospect_rsi_valleys.xlsx', index = True )\n",
        "prospect_list_peaks_df.to_excel('prospect_rsi_peaks.xlsx', index = False)"
      ],
      "metadata": {
        "id": "iuostsU0BBkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define the directory containing the CSV files\n",
        "csv_directory = '/content/Data'  # Update this path as your file\n",
        "\n",
        "# List of CSV files for different sectors and industries\n",
        "sector_industry_files = ['/content/Data/ind_nifty_privatebanklist.csv', '/content/Data/ind_niftyautolist.csv', '/content/Data/ind_niftybanklist.csv',\n",
        "'/content/Data/ind_niftyconsumerdurableslist.csv','/content/Data/ind_niftyfinancelist.csv', '/content/Data/ind_niftyfmcglist.csv',\n",
        "'/content/Data/ind_niftyhealthcarelist.csv','/content/Data/ind_niftyitlist.csv' , '/content/Data/ind_niftymedialist.csv',\n",
        "'/content/Data/ind_niftymetallist.csv', '/content/Data/ind_niftyoilgaslist.csv', '/content/Data/ind_niftypharmalist.csv',\n",
        "'/content/Data/ind_niftypsubanklist.csv', '/content/Data/ind_niftyrealtylist.csv' ]\n",
        "\n",
        "\n",
        "\n",
        "nifty500_file = '/content/ind_nifty500list.csv'\n",
        "medium_cap_file = '/content/ind_niftymidcap150list.csv'\n",
        "small_cap_file = '/content/ind_niftysmallcap250list.csv'\n",
        "\n",
        "# Read Nifty 500 CSV file\n",
        "nifty500_df = pd.read_csv(os.path.join(csv_directory, nifty500_file))\n",
        "nifty500_symbols = nifty500_df['Symbol'].tolist()\n",
        "\n",
        "# Read Cap CSV files\n",
        "medium_cap_df = pd.read_csv(os.path.join(csv_directory, medium_cap_file))\n",
        "small_cap_df = pd.read_csv(os.path.join(csv_directory, small_cap_file))\n",
        "\n",
        "# Function to determine cap\n",
        "def determine_cap(symbol):\n",
        "    if symbol in medium_cap_df['Symbol'].tolist():\n",
        "        return 'Medium'\n",
        "    elif symbol in small_cap_df['Symbol'].tolist():\n",
        "        return 'Small'\n",
        "    else:\n",
        "        return 'None'\n",
        "\n",
        "# Consolidate data\n",
        "consolidated_data = []\n",
        "\n",
        "for file in sector_industry_files:\n",
        "    sector_industry_df = pd.read_csv(os.path.join(csv_directory, file))\n",
        "    industry = sector_industry_df['Industry'][0]  # Assuming the industry is the same for all rows in this file\n",
        "\n",
        "    for symbol in sector_industry_df['Symbol']:\n",
        "        nifty500_status = 'Nifty 500' if symbol in nifty500_symbols else 'None'\n",
        "        cap = determine_cap(symbol)\n",
        "\n",
        "        consolidated_data.append({\n",
        "            'Symbol': symbol,\n",
        "            'Nifty 500': nifty500_status,\n",
        "            'Industry': industry,\n",
        "            'Cap': cap\n",
        "        })\n",
        "\n",
        "# Create a DataFrame from the consolidated data\n",
        "consolidated_df = pd.DataFrame(consolidated_data)\n",
        "\n",
        "# Save the consolidated DataFrame to an Excel file\n",
        "output_file = 'newconsolidated_data.xlsx'\n",
        "consolidated_df.to_excel(output_file, index=False)\n",
        "\n",
        "print(\"Consolidated data has been saved to\", output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tG3tZU8BctLs",
        "outputId": "60f2881e-0da6-442c-8a8d-7a7247408906"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consolidated data has been saved to newconsolidated_data.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# List of CSV files for different sectors and industries\n",
        "sector_industry_files = ['/content/Data/ind_nifty_privatebanklist.csv', '/content/Data/ind_niftyautolist.csv', '/content/Data/ind_niftybanklist.csv',\n",
        "'/content/Data/ind_niftyconsumerdurableslist.csv','/content/Data/ind_niftyfinancelist.csv', '/content/Data/ind_niftyfmcglist.csv',\n",
        "'/content/Data/ind_niftyhealthcarelist.csv','/content/Data/ind_niftyitlist.csv' , '/content/Data/ind_niftymedialist.csv',\n",
        "'/content/Data/ind_niftymetallist.csv', '/content/Data/ind_niftyoilgaslist.csv', '/content/Data/ind_niftypharmalist.csv',\n",
        "'/content/Data/ind_niftypsubanklist.csv', '/content/Data/ind_niftyrealtylist.csv' ]\n",
        "\n",
        "nifty500_file = '/content/ind_nifty500list.csv'\n",
        "medium_cap_file = '/content/ind_niftymidcap150list.csv'\n",
        "small_cap_file = '/content/ind_niftysmallcap250list.csv'\n",
        "\n",
        "# Read Nifty 500 CSV file\n",
        "nifty500_df = pd.read_csv(nifty500_file)\n",
        "nifty500_symbols = nifty500_df['Symbol'].tolist()\n",
        "\n",
        "# Read Cap CSV files\n",
        "medium_cap_df = pd.read_csv(medium_cap_file)\n",
        "small_cap_df = pd.read_csv(small_cap_file)\n",
        "\n",
        "# Function to determine cap\n",
        "def determine_cap(symbol):\n",
        "    if symbol in medium_cap_df['Symbol'].tolist():\n",
        "        return 'Medium'\n",
        "    elif symbol in small_cap_df['Symbol'].tolist():\n",
        "        return 'Small'\n",
        "    else:\n",
        "        return 'None'\n",
        "\n",
        "# Consolidate data\n",
        "consolidated_data = []\n",
        "\n",
        "for file in sector_industry_files:\n",
        "    sector_industry_df = pd.read_csv(file)\n",
        "    industry = sector_industry_df['Industry'][0]  # Assuming the industry is the same for all rows in this file\n",
        "\n",
        "    for symbol in sector_industry_df['Symbol']:\n",
        "        nifty500_status = 'Nifty 500' if symbol in nifty500_symbols else 'None'\n",
        "        cap = determine_cap(symbol)\n",
        "\n",
        "        consolidated_data.append({\n",
        "            'Symbol': symbol,\n",
        "            'Nifty 500': nifty500_status,\n",
        "            'Industry': industry,\n",
        "            'Cap': cap\n",
        "        })\n",
        "\n",
        "# Create a DataFrame from the consolidated data\n",
        "consolidated_df = pd.DataFrame(consolidated_data)\n",
        "\n",
        "# Save the consolidated DataFrame to an Excel file\n",
        "output_file = 'consolidated_data.xlsx'\n",
        "consolidated_df.to_excel(output_file, index=False)\n",
        "\n",
        "print(\"Consolidated data has been saved to\", output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBAsjNkuY4pD",
        "outputId": "8cedd6f2-a54b-4afd-e34f-53572179a083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consolidated data has been saved to consolidated_data.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define the directory containing the CSV files\n",
        "csv_directory = '/content/Data'  # Update this path as your file\n",
        "\n",
        "# List of CSV files for different sectors and industries\n",
        "sector_industry_files = [\n",
        "    'ind_nifty_privatebanklist.csv', 'ind_niftyautolist.csv', 'ind_niftybanklist.csv',\n",
        "    'ind_niftyconsumerdurableslist.csv', 'ind_niftyfinancelist.csv', 'ind_niftyfmcglist.csv',\n",
        "    'ind_niftyhealthcarelist.csv', 'ind_niftyitlist.csv', 'ind_niftymedialist.csv',\n",
        "    'ind_niftymetallist.csv', 'ind_niftyoilgaslist.csv', 'ind_niftypharmalist.csv',\n",
        "    'ind_niftypsubanklist.csv', 'ind_niftyrealtylist.csv'\n",
        "]\n",
        "\n",
        "\n",
        "# Directory containing theme sector files\n",
        "theme_sector_directory = '/content/ThemeData'  # Update this path with your actual theme sector directory\n",
        "\n",
        "nifty50_file = '/content/ind_nifty50list.csv'\n",
        "nifty100_file = '/content/ind_nifty100list.csv'\n",
        "nifty500_file = '/content/ind_nifty500list.csv'\n",
        "medium_cap_file = '/content/ind_niftymidcap150list.csv'\n",
        "small_cap_file = '/content/ind_niftysmallcap250list.csv'\n",
        "\n",
        "\n",
        "# Read Nifty 500 CSV file\n",
        "nifty500_df = pd.read_csv(os.path.join(csv_directory, nifty500_file))\n",
        "nifty500_symbols = nifty500_df['Symbol'].tolist()\n",
        "\n",
        "# Read Cap CSV files\n",
        "medium_cap_df = pd.read_csv(os.path.join(csv_directory, medium_cap_file))\n",
        "small_cap_df = pd.read_csv(os.path.join(csv_directory, small_cap_file))\n",
        "\n",
        "# Read Nifty50 and Nifty100 CSV files\n",
        "nifty50_df = pd.read_csv(os.path.join(csv_directory, nifty50_file))\n",
        "nifty100_df = pd.read_csv(os.path.join(csv_directory, nifty100_file))\n",
        "nifty50_symbols = nifty50_df['Symbol'].tolist()\n",
        "nifty100_symbols = nifty100_df['Symbol'].tolist()\n",
        "\n",
        "# Read theme sector files\n",
        "theme_sector_data = {}\n",
        "for file in os.listdir(theme_sector_directory):\n",
        "    if file.endswith('.csv'):\n",
        "        theme_df = pd.read_csv(os.path.join(theme_sector_directory, file))\n",
        "        theme_sector = os.path.splitext(file)[0]  # Get the theme sector name from the file name\n",
        "        for _, row in theme_df.iterrows():\n",
        "            symbol = row['Symbol']\n",
        "            industry = row['Industry']\n",
        "            theme_sector_data[symbol] = (industry, theme_sector)\n",
        "\n",
        "# Function to determine cap\n",
        "def determine_cap(symbol):\n",
        "    if symbol in medium_cap_df['Symbol'].tolist():\n",
        "        return 'Medium'\n",
        "    elif symbol in small_cap_df['Symbol'].tolist():\n",
        "        return 'Small'\n",
        "    else:\n",
        "        return 'None'\n",
        "\n",
        "# Consolidate data\n",
        "consolidated_data = []\n",
        "\n",
        "for file in sector_industry_files:\n",
        "    sector_industry_df = pd.read_csv(os.path.join(csv_directory, file))\n",
        "    industry = sector_industry_df['Industry'][0]  # Assuming the industry is the same for all rows in this file\n",
        "    sector = os.path.splitext(file)[0]  # Get the sector name from the file name\n",
        "\n",
        "    for symbol in sector_industry_df['Symbol']:\n",
        "        nifty500_status = 'True' if symbol in nifty500_symbols else 'False'\n",
        "        cap = determine_cap(symbol)\n",
        "        nifty50_status = 'True' if symbol in nifty50_symbols else 'False'\n",
        "        nifty100_status = 'True' if symbol in nifty100_symbols else 'False'\n",
        "\n",
        "        theme_industry, theme_sector = theme_sector_data.get(symbol, ('None', 'None'))\n",
        "\n",
        "        consolidated_data.append({\n",
        "            'Symbol': symbol,\n",
        "            'Nifty50': nifty50_status,\n",
        "            'Nifty100': nifty100_status,\n",
        "            'Nifty 500': nifty500_status,\n",
        "            'Industry': industry,\n",
        "            'Cap': cap,\n",
        "            'Sector': sector,\n",
        "            'Theme_Industry': theme_industry,\n",
        "            'Theme_Sector': theme_sector\n",
        "        })\n",
        "\n",
        "# Create a DataFrame from the consolidated data\n",
        "consolidated_df = pd.DataFrame(consolidated_data)\n",
        "\n",
        "# Save the consolidated DataFrame to an Excel file\n",
        "output_file = 'finalconsolidated_data.xlsx'\n",
        "consolidated_df.to_excel(output_file, index=False)\n",
        "\n",
        "print(\"Consolidated data has been saved to\", output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diA1QGnaR5FQ",
        "outputId": "7cdbdf28-8e72-4861-c4c3-b0a7bfc9252c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consolidated data has been saved to finalconsolidated_data.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1967 code2\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define the directory containing the CSV files\n",
        "csv_directory = '/content/Industry'  # Update this path as needed\n",
        "\n",
        "# List of CSV files for different sectors and industries\n",
        "sector_industry_files = [\n",
        "    'ind_nifty_privatebanklist.csv', 'ind_niftyautolist.csv', 'ind_niftybanklist.csv',\n",
        "    'ind_niftyconsumerdurableslist.csv', 'ind_niftyfinancelist.csv', 'ind_niftyfmcglist.csv',\n",
        "    'ind_niftyhealthcarelist.csv', 'ind_niftyitlist.csv', 'ind_niftymedialist.csv',\n",
        "    'ind_niftymetallist.csv', 'ind_niftyoilgaslist.csv', 'ind_niftypharmalist.csv',\n",
        "    'ind_niftypsubanklist.csv', 'ind_niftyrealtylist.csv'\n",
        "]\n",
        "\n",
        "# List of CSV files for theme sectors\n",
        "# List of CSV files for theme sectors\n",
        "theme_sector_directory = '/content/Theme'\n",
        "\n",
        "# Read NSE CSV file\n",
        "nse_file = '/content/NSE_stocks.csv'\n",
        "nse_df = pd.read_csv(os.path.join(csv_directory, nse_file))\n",
        "nse_symbols = nse_df['SYMBOL'].tolist()\n",
        "\n",
        "# Read Cap CSV files\n",
        "medium_cap_file = '/content/ind_niftymidcap150list.csv'\n",
        "small_cap_file = '/content/ind_niftysmallcap250list.csv'\n",
        "medium_cap_df = pd.read_csv(os.path.join(csv_directory, medium_cap_file))\n",
        "small_cap_df = pd.read_csv(os.path.join(csv_directory, small_cap_file))\n",
        "\n",
        "# Read Nifty50 and Nifty100 CSV files\n",
        "nifty50_file = '/content/ind_nifty50list.csv'\n",
        "nifty100_file = '/content/ind_nifty100list.csv'\n",
        "nifty500_file = '/content/ind_nifty500list.csv'\n",
        "nifty50_df = pd.read_csv(os.path.join(csv_directory, nifty50_file))\n",
        "nifty100_df = pd.read_csv(os.path.join(csv_directory, nifty100_file))\n",
        "nifty500_df = pd.read_csv(os.path.join(csv_directory, nifty500_file))\n",
        "nifty50_symbols = nifty50_df['Symbol'].tolist()\n",
        "nifty100_symbols = nifty100_df['Symbol'].tolist()\n",
        "nifty500_symbols = nifty500_df['Symbol'].tolist()\n",
        "\n",
        "# Read theme sector files\n",
        "theme_sector_data = {}\n",
        "for file in os.listdir(theme_sector_directory):\n",
        "    if file.endswith('.csv'):\n",
        "        theme_df = pd.read_csv(os.path.join(theme_sector_directory, file))\n",
        "        theme_sector = os.path.splitext(file)[0]  # Get the theme sector name from the file name\n",
        "        for _, row in theme_df.iterrows():\n",
        "            symbol = row['Symbol']\n",
        "            industry = row['Industry']\n",
        "            theme_sector_data[symbol] = (industry, theme_sector)\n",
        "\n",
        "# Function to determine cap\n",
        "def determine_cap(symbol):\n",
        "    if symbol in medium_cap_df['Symbol'].tolist():\n",
        "        return 'Medium'\n",
        "    elif symbol in small_cap_df['Symbol'].tolist():\n",
        "        return 'Small'\n",
        "    else:\n",
        "        return 'None'\n",
        "\n",
        "# Consolidate data\n",
        "consolidated_data = []\n",
        "\n",
        "for symbol in nse_symbols:\n",
        "    nifty500_status = 'Nifty 500' if symbol in nifty500_symbols else 'None'\n",
        "    cap = determine_cap(symbol)\n",
        "    nifty50_status = 'True' if symbol in nifty50_symbols else 'False'\n",
        "    nifty100_status = 'True' if symbol in nifty100_symbols else 'False'\n",
        "\n",
        "    industry, sector = 'None', 'None'\n",
        "    for file in sector_industry_files:\n",
        "        sector_industry_df = pd.read_csv(os.path.join(csv_directory, file))\n",
        "        if symbol in sector_industry_df['Symbol'].tolist():\n",
        "            industry = sector_industry_df.loc[sector_industry_df['Symbol'] == symbol, 'Industry'].values[0]\n",
        "            sector = os.path.splitext(file)[0]\n",
        "            break\n",
        "\n",
        "    theme_industry, theme_sector = theme_sector_data.get(symbol, ('None', 'None'))\n",
        "\n",
        "    consolidated_data.append({\n",
        "        'Symbol': symbol,\n",
        "        'Nifty 500': nifty500_status,\n",
        "        'Industry': industry,\n",
        "        'Cap': cap,\n",
        "        'Sector': sector,\n",
        "        'Nifty50': nifty50_status,\n",
        "        'Nifty100': nifty100_status,\n",
        "        'Theme_Industry': theme_industry,\n",
        "        'Theme_Sector': theme_sector\n",
        "    })\n",
        "\n",
        "# Create a DataFrame from the consolidated data\n",
        "consolidated_df = pd.DataFrame(consolidated_data)\n",
        "\n",
        "# Save the consolidated DataFrame to an Excel file\n",
        "output_file = '1967symbols_data.xlsx'\n",
        "consolidated_df.to_excel(output_file, index=False)\n",
        "\n",
        "print(\"Consolidated data has been saved to\", output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKGk3dktlUUZ",
        "outputId": "793fc7ec-3724-45e3-f9a0-f64f0b7e8710"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consolidated data has been saved to 1967symbols_data.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define the directory containing the CSV files\n",
        "csv_directory = '/content/Data'  # Update this path as your file\n",
        "\n",
        "# List of CSV files for different sectors and industries\n",
        "sector_industry_files = [\n",
        "    'ind_nifty_privatebanklist.csv', 'ind_niftyautolist.csv', 'ind_niftybanklist.csv',\n",
        "    'ind_niftyconsumerdurableslist.csv', 'ind_niftyfinancelist.csv', 'ind_niftyfmcglist.csv',\n",
        "    'ind_niftyhealthcarelist.csv', 'ind_niftyitlist.csv', 'ind_niftymedialist.csv',\n",
        "    'ind_niftymetallist.csv', 'ind_niftyoilgaslist.csv', 'ind_niftypharmalist.csv',\n",
        "    'ind_niftypsubanklist.csv', 'ind_niftyrealtylist.csv'\n",
        "]\n",
        "\n",
        "# Directory containing theme sector files\n",
        "theme_sector_directory = '/content/ThemeData'  # Update this path with your actual theme sector directory\n",
        "\n",
        "nifty50_file = '/content/ind_nifty50list.csv'\n",
        "nifty100_file = '/content/ind_nifty100list.csv'\n",
        "nifty500_file = '/content/ind_nifty500list.csv'\n",
        "medium_cap_file = '/content/ind_niftymidcap150list.csv'\n",
        "small_cap_file = '/content/ind_niftysmallcap250list.csv'\n",
        "\n",
        "\n",
        "# Read Nifty 500 CSV file\n",
        "nifty500_df = pd.read_csv(os.path.join(csv_directory, nifty500_file))\n",
        "nifty500_symbols = nifty500_df['Symbol'].tolist()\n",
        "\n",
        "# Read Cap CSV files\n",
        "medium_cap_df = pd.read_csv(os.path.join(csv_directory, medium_cap_file))\n",
        "small_cap_df = pd.read_csv(os.path.join(csv_directory, small_cap_file))\n",
        "\n",
        "# Read Nifty50 and Nifty100 CSV files\n",
        "nifty50_df = pd.read_csv(os.path.join(csv_directory, nifty50_file))\n",
        "nifty100_df = pd.read_csv(os.path.join(csv_directory, nifty100_file))\n",
        "nifty50_symbols = nifty50_df['Symbol'].tolist()\n",
        "nifty100_symbols = nifty100_df['Symbol'].tolist()\n",
        "\n",
        "# Read theme sector files\n",
        "theme_sector_data = {}\n",
        "for file in os.listdir(theme_sector_directory):\n",
        "    if file.endswith('.csv'):\n",
        "        theme_df = pd.read_csv(os.path.join(theme_sector_directory, file))\n",
        "        theme_sector = os.path.splitext(file)[0]  # Get the theme sector name from the file name\n",
        "        for _, row in theme_df.iterrows():\n",
        "            symbol = row['Symbol']\n",
        "            industry = row['Industry']\n",
        "            theme_sector_data[symbol] = (industry, theme_sector)\n",
        "\n",
        "# Function to determine cap\n",
        "def determine_cap(symbol):\n",
        "    if symbol in medium_cap_df['Symbol'].tolist():\n",
        "        return 'Medium'\n",
        "    elif symbol in small_cap_df['Symbol'].tolist():\n",
        "        return 'Small'\n",
        "    else:\n",
        "        return 'None'\n",
        "\n",
        "# Function to calculate RSI\n",
        "def calculate_rsi(prices, window=14):\n",
        "    delta = prices.diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
        "    rs = gain / loss\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    return rsi\n",
        "\n",
        "# Function to find peaks and valleys\n",
        "def find_peaks_and_valleys(rsi_series):\n",
        "    peaks = []\n",
        "    valleys = []\n",
        "    for i in range(1, len(rsi_series) - 1):\n",
        "        if rsi_series[i] > rsi_series[i - 1] and rsi_series[i] > rsi_series[i + 1]:\n",
        "            peaks.append(rsi_series[i])\n",
        "        elif rsi_series[i] < rsi_series[i - 1] and rsi_series[i] < rsi_series[i + 1]:\n",
        "            valleys.append(rsi_series[i])\n",
        "    return peaks, valleys\n",
        "\n",
        "# Consolidate data\n",
        "consolidated_data = []\n",
        "\n",
        "for file in sector_industry_files:\n",
        "    sector_industry_df = pd.read_csv(os.path.join(csv_directory, file))\n",
        "    industry = sector_industry_df['Industry'][0]  # Assuming the industry is the same for all rows in this file\n",
        "    sector = os.path.splitext(file)[0]  # Get the sector name from the file name\n",
        "\n",
        "    for symbol in sector_industry_df['Symbol']:\n",
        "        nifty500_status = 'Nifty 500' if symbol in nifty500_symbols else 'None'\n",
        "        cap = determine_cap(symbol)\n",
        "        nifty50_status = 'True' if symbol in nifty50_symbols else 'False'\n",
        "        nifty100_status = 'True' if symbol in nifty100_symbols else 'False'\n",
        "\n",
        "        theme_industry, theme_sector = theme_sector_data.get(symbol, ('None', 'None'))\n",
        "\n",
        "        # Assuming we have a DataFrame `rsi_data` with 'Symbol' and 'Close' prices for calculating RSI\n",
        "        rsi_data = pd.DataFrame({\n",
        "            'Symbol': [symbol] * 100,  # Replace with your actual data source\n",
        "            'CH_CLOSING_PRICE': pd.Series(range(100))  # Replace with actual closing prices\n",
        "        })\n",
        "\n",
        "        if symbol in rsi_data['Symbol'].unique():\n",
        "            symbol_rsi_data = rsi_data[rsi_data['Symbol'] == symbol]\n",
        "            symbol_rsi_data['RSI'] = calculate_rsi(symbol_rsi_data['CH_CLOSING_PRICE'])\n",
        "            peaks, valleys = find_peaks_and_valleys(symbol_rsi_data['RSI'])\n",
        "        else:\n",
        "            peaks, valleys = [], []\n",
        "\n",
        "        consolidated_data.append({\n",
        "            'Symbol': symbol,\n",
        "            'Nifty 500': nifty500_status,\n",
        "            'Industry': industry,\n",
        "            'Cap': cap,\n",
        "            'Sector': sector,\n",
        "            'Nifty50': nifty50_status,\n",
        "            'Nifty100': nifty100_status,\n",
        "            'Theme_Industry': theme_industry,\n",
        "            'Theme_Sector': theme_sector,\n",
        "            'RSI_Peak': peaks,\n",
        "            'RSI_Valleys': valleys\n",
        "        })\n",
        "\n",
        "# Create a DataFrame from the consolidated data\n",
        "consolidated_df = pd.DataFrame(consolidated_data)\n",
        "\n",
        "# Save the consolidated DataFrame to an Excel file\n",
        "output_file = 'rsi_consolidated_data.xlsx'\n",
        "consolidated_df.to_excel(output_file, index=False)\n",
        "\n",
        "print(\"Consolidated data has been saved to\", output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFdwES6YvnRt",
        "outputId": "b3f00dc9-d12b-46f3-86c2-0da5e8eeaa81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consolidated data has been saved to rsi_consolidated_data.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import yfinance as yf\n",
        "\n",
        "# Define the directory containing the CSV files\n",
        "csv_directory = '/content/Data'  # Update this path as your file\n",
        "\n",
        "# List of CSV files for different sectors and industries\n",
        "sector_industry_files = [\n",
        "    'ind_nifty_privatebanklist.csv', 'ind_niftyautolist.csv', 'ind_niftybanklist.csv',\n",
        "    'ind_niftyconsumerdurableslist.csv', 'ind_niftyfinancelist.csv', 'ind_niftyfmcglist.csv',\n",
        "    'ind_niftyhealthcarelist.csv', 'ind_niftyitlist.csv', 'ind_niftymedialist.csv',\n",
        "    'ind_niftymetallist.csv', 'ind_niftyoilgaslist.csv', 'ind_niftypharmalist.csv',\n",
        "    'ind_niftypsubanklist.csv', 'ind_niftyrealtylist.csv'\n",
        "]\n",
        "\n",
        "# Directory containing theme sector files\n",
        "theme_sector_directory = '/content/ThemeData'  # Update this path with your actual theme sector directory\n",
        "\n",
        "nifty500_file = '/content/ind_nifty500list.csv'\n",
        "medium_cap_file = '/content/ind_niftymidcap150list.csv'\n",
        "small_cap_file = '/content/ind_niftymidcap150list.csv'\n",
        "nifty50_file = '/content/ind_nifty50list.csv'\n",
        "nifty100_file = '/content/ind_nifty100list.csv'\n",
        "\n",
        "# Read Nifty 500 CSV file\n",
        "nifty500_df = pd.read_csv(os.path.join(csv_directory, nifty500_file))\n",
        "nifty500_symbols = nifty500_df['Symbol'].tolist()\n",
        "\n",
        "# Read Cap CSV files\n",
        "medium_cap_df = pd.read_csv(os.path.join(csv_directory, medium_cap_file))\n",
        "small_cap_df = pd.read_csv(os.path.join(csv_directory, small_cap_file))\n",
        "\n",
        "# Read Nifty50 and Nifty100 CSV files\n",
        "nifty50_df = pd.read_csv(os.path.join(csv_directory, nifty50_file))\n",
        "nifty100_df = pd.read_csv(os.path.join(csv_directory, nifty100_file))\n",
        "nifty50_symbols = nifty50_df['Symbol'].tolist()\n",
        "nifty100_symbols = nifty100_df['Symbol'].tolist()\n",
        "\n",
        "# Read theme sector files\n",
        "theme_sector_data = {}\n",
        "for file in os.listdir(theme_sector_directory):\n",
        "    if file.endswith('.csv'):\n",
        "        theme_df = pd.read_csv(os.path.join(theme_sector_directory, file))\n",
        "        theme_sector = os.path.splitext(file)[0]  # Get the theme sector name from the file name\n",
        "        for _, row in theme_df.iterrows():\n",
        "            symbol = row['Symbol']\n",
        "            industry = row['Industry']\n",
        "            theme_sector_data[symbol] = (industry, theme_sector)\n",
        "\n",
        "# Function to determine cap\n",
        "def determine_cap(symbol):\n",
        "    if symbol in medium_cap_df['Symbol'].tolist():\n",
        "        return 'Medium'\n",
        "    elif symbol in small_cap_df['Symbol'].tolist():\n",
        "        return 'Small'\n",
        "    else:\n",
        "        return 'None'\n",
        "\n",
        "# Function to calculate RSI\n",
        "def calculate_rsi(prices, window=14):\n",
        "    delta = prices.diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
        "    rs = gain / loss\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    return rsi\n",
        "\n",
        "# Function to find peaks and valleys\n",
        "def find_peaks_and_valleys(rsi_series):\n",
        "    peaks = []\n",
        "    valleys = []\n",
        "    for i in range(1, len(rsi_series) - 1):\n",
        "        if rsi_series[i] > rsi_series[i - 1] and rsi_series[i] > rsi_series[i + 1]:\n",
        "            peaks.append(rsi_series[i])\n",
        "        elif rsi_series[i] < rsi_series[i - 1] and rsi_series[i] < rsi_series[i + 1]:\n",
        "            valleys.append(rsi_series[i])\n",
        "    return peaks, valleys\n",
        "\n",
        "# Function to fetch historical data and calculate RSI for a symbol\n",
        "def get_rsi_for_symbol(symbol, period='1y', interval='1d'):\n",
        "    try:\n",
        "        symbol_yf = symbol + '.NS'\n",
        "        data = yf.download(symbol_yf, period=period, interval=interval)\n",
        "        data['RSI'] = calculate_rsi(data['Close'])\n",
        "        return data['RSI']\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching data for {symbol}: {e}\")\n",
        "        return pd.Series()\n",
        "\n",
        "# Consolidate data\n",
        "consolidated_data = []\n",
        "\n",
        "for file in sector_industry_files:\n",
        "    sector_industry_df = pd.read_csv(os.path.join(csv_directory, file))\n",
        "    industry = sector_industry_df['Industry'][0]  # Assuming the industry is the same for all rows in this file\n",
        "    sector = os.path.splitext(file)[0]  # Get the sector name from the file name\n",
        "\n",
        "    for symbol in sector_industry_df['Symbol']:\n",
        "        nifty500_status = 'Nifty 500' if symbol in nifty500_symbols else 'None'\n",
        "        cap = determine_cap(symbol)\n",
        "        nifty50_status = 'True' if symbol in nifty50_symbols else 'False'\n",
        "        nifty100_status = 'True' if symbol in nifty100_symbols else 'False'\n",
        "\n",
        "        theme_industry, theme_sector = theme_sector_data.get(symbol, ('None', 'None'))\n",
        "\n",
        "        rsi_series = get_rsi_for_symbol(symbol)\n",
        "        peaks, valleys = find_peaks_and_valleys(rsi_series)\n",
        "\n",
        "        consolidated_data.append({\n",
        "            'Symbol': symbol,\n",
        "            'Nifty50': nifty50_status,\n",
        "            'Nifty100': nifty100_status,\n",
        "            'Nifty 500': nifty500_status,\n",
        "            'Industry': industry,\n",
        "            'Cap': cap,\n",
        "            'Sector': sector,\n",
        "            'Theme_Industry': theme_industry,\n",
        "            'Theme_Sector': theme_sector,\n",
        "            'RSI_Peak': peaks,\n",
        "            'RSI_Valleys': valleys\n",
        "        })\n",
        "\n",
        "# Create a DataFrame from the consolidated data\n",
        "consolidated_df = pd.DataFrame(consolidated_data)\n",
        "\n",
        "# Save the consolidated DataFrame to an Excel file\n",
        "output_file = 'rsi_consolidated_data.xlsx'\n",
        "consolidated_df.to_excel(output_file, index=False)\n",
        "\n",
        "print(\"Consolidated data has been saved to\", output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ddx9dnw-0Tiz",
        "outputId": "cdf204f9-1337-4dc2-bcf8-67a796aeda2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consolidated data has been saved to rsi_consolidated_data.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ta\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnoE1nTU29d1",
        "outputId": "8acbf551-730c-4d67-e007-06bc696c41ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ta\n",
            "  Downloading ta-0.11.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ta) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ta) (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->ta) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ta) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ta) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->ta) (1.16.0)\n",
            "Building wheels for collected packages: ta\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29411 sha256=3fd23e2a74ba7b1106b2b9d84adb3ee7febf9b293efc85994e6357462d861293\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/67/4f/8a9f252836e053e532c6587a3230bc72a4deb16b03a829610b\n",
            "Successfully built ta\n",
            "Installing collected packages: ta\n",
            "Successfully installed ta-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yfinance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HVuWesLaBgu",
        "outputId": "266669c6-9014-4917-a16b-fa3c8a41ff47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.10/dist-packages (0.2.40)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.0.3)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.25.2)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.31.0)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.9.4)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.2.2)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2023.4)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.4.4)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.10/dist-packages (from yfinance) (3.17.5)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.12.3)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->yfinance) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (2024.6.2)\n"
          ]
        }
      ]
    }
  ]
}